\documentclass[a4paper]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[T1]{fontenc}
%\usepackage{geometry}
\usepackage{graphicx}
\usepackage[utf8x]{inputenc}
\usepackage{listings}
\usepackage{mathdots}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{mdframed}
\usepackage{microtype}
\usepackage{multirow}
\usepackage[round,authoryear]{natbib}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{relsize}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{stmaryrd}
\usepackage{tabularx}
\usepackage{tcolorbox}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{url}

% Configure natbib
\bibliographystyle{unsrtnat.bst}

\DeclareMathOperator*{Pr}{Pr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand*\acr[1]{\textscale{.85}{#1}} % Custom acronyms command

% see: https://www.bu.edu/math/files/2013/08/tikzpgfmanual.pdf, section 16.6, p. 161
% awesome macros here: http://gnuplot.sourceforge.net/demo/prob.20.gnu
\usetikzlibrary{positioning}
\usepgfplotslibrary{external}
\texexternalize
\tikzstyle{every plot}=[prefix=plots/]


% some random definitions, you can delete these!
\def\isint(#1){(int(#1)==#1)}
\def\Binv(#1)(#2){exp(lgamma(#1+#2)-lgamma(#1)-lgamma(#2))}
\def\gmm(#1)(#2)(#3){#2<=0||#3<=0?1/0:#1<0?0.0:#1==0?(#2>1?0.0:#2==1?real(#3):1/0):exp(#2*log(#3)+(#2-1.0)*log(#1)-lgamma(#2)-#3*#1)}


\title{Bayesian Estimation of Distributions}
% \author{Maxime Robeyns}

\begin{document}

\maketitle

The following is an approach to finding a parametric estimate of a conditional
distribution.

% Our goal in distributional reinforcement learning is to obtain a maginal
% distribution for the returns given some state action pair:
% \[
%   P(R_t\vert \theta, s_t, a_t),
% \]
% for which we use some parametric function approximator.

The approach described below is loosely inspired from quantile regression, which
we review here briefly.

\section{Brief Review of Quantile Regression}

If $Y$ is a real-valued random variable with cumulative distribution function
$F_Y(y) = P(Y\le y)$, then the $\tau$th quantile of $Y$ is given by the
quantile function
\[
  Q_Y(\tau) = F^{-1}_Y(\tau) = \inf\{y:F_Y(y) \ge \tau\},
\]
for some selected quantile $\tau \in (0,1)$. The loss function used in quantile
regression, due to \cite{koenkerRegressionQuantiles1978}, is defined as
\begin{equation}\label{eq:qrl}
  \rho_\tau(u) = u(\tau - \mathbb{I}_{(u<0)}),
\end{equation}
for $\mathbb{I}$ an indicator function.
To see that we may obtain the $\tau$th quantile of $Y$ by minimising the above,
consider an estimate $\hat{y}$ for this quantile. We seek
\begin{align}
  \min_{\hat{y}}\mathbb{E}\big[\rho_\tau(Y - \hat{y})\big] &=
  \min_{\hat{y}}\int_{-\infty}^{\infty} \rho_\tau(y-\hat{y})dF_Y(y) \nonumber \\
                                                           &=
  \min_{\hat{y}}\left\{(\tau - 1) \int^{\hat{y}}_{-\infty}(y-\hat{y})dF_Y(y) +
  \tau\int^{\infty}_{\hat{y}}(y-\hat{y})dF_Y(y)\right\}. \label{eq:exploss}
\end{align}
Using Leibniz's rule, we can differentiate this expression with respect to
$\hat{y}$, which gives
\begin{align*}
  \frac{d}{d\hat{y}} \mathbb{E}\big[\rho_\tau(Y-\hat{y})\big] &= \frac{d}{dx}
  (\tau - 1)\int_{-\infty}^{\hat{y}}(y-\hat{y})dF_Y(y) +
  \tau\int^{\infty}_{\hat{y}}(y-\hat{y})F_Y(y) \\
  &= (\tau - 1)\left((\hat{y}-\hat{y})+\int^{\hat{y}}_{-\infty}\frac{\delta}{\delta \hat{y}}(y-\hat{y})dF_Y(y)\right) \\
  &\ \ \ \ + \tau \left(-(\hat{y}-\hat{y}) + \int^{\infty}_{\hat{y}}\frac{\delta}{\delta \hat{y}}(y-\hat{y})dF_Y(y)\right) \\
  &= (1-\tau) F_Y(\hat{y}) - \tau \big(1- F_Y(\hat{y})\big) \\
  &= F_Y(\hat{y}) - \tau.
\end{align*}
Since both of the terms in (\ref{eq:exploss}) are positive, the loss function is
convex and so setting the above expression equal to zero and solving gives
$F_Y(\hat{y}) = \tau$. Hence $\hat{y}$ is indeed the $\tau$th quantile of the
random variable $Y$, as required.

In the approach of quantile regression, we seek to approximate the $\tau$th conditional
quantile function, $Q_{Y}(\tau|X) = f_{\mathbf{w}_\tau}(X)$, where
$f_{\mathbf{w}_\tau}(X)$ is a parametric function approximator with parameters
$\mathbf{w}_\tau$. We may obtain the parameters $\mathbf{w}_\tau$ by solving:
\[
  \mathbf{w}_{\tau} = \argmin_{\mathbf{w}\in\mathbb{R}^d} \mathbb{E}\big[\rho_\tau(Y
- f_{\mathbf{w}}(X))\big].
\]
Given that we usually don't have the distribution function $F_Y(y)$ of $Y$
available and we are instead estimating the parameter values from data, we
may instead estimate the parameters from the observations using
\[
  \hat{\mathbf{w}}_{\tau} =
  \argmin_{\mathbf{w}\in\mathbb{R}^d}\sum^n_{i=1}\rho_\tau(y_i -
  f_{\mathbf{w}}\big(\mathbf{x}_i)\big).
\]

For a probabilistic analysis of the above, since the quantile regression loss
function is always positive, we may re-frame it as a member of the exponential
family of distributions by simply taking its negative exponent. This gives an
asymmetric Laplace density of the following form
\citep{yuBayesianQuantileRegression2001}
\[
  e^{-\mathbb{E}\big[\rho_\tau(Y - f_{\mathbf{w}_\tau}(X))\big]} =
  \frac{\tau^n(1-\tau)^n}{\sigma}\exp\left\{-\sum^N_{i=1}\rho_\tau\left(\frac{y_i
  - f_{\mathbf{w}_\tau}(\mathbf{x}_i)}{\sigma}\right)\right\},
\]
for some scale parameter $\sigma$, and $\tau$ acting as the asymmetry parameter.
For modelling the median, $\tau = 0.5$ and this becomes equivalent to the
Laplace distribution. This can be treated as a likelihood function,
and maximising this is an effective approach to finding the parameters
$\mathbf{w}_\tau$ which give the $\tau$th conditional quantile.

However a single $\tau$ fit doesn't do justice to the potential of this
framework which is to model all quantiles simultaneously; that is, $Q_Y(\tau
\vert X)$ for all $\tau \in (0,1)$.
For this, we need a different approach.

% For this, the asymmetric Laplace likelihood
% is insufficient. Modelling different quantiles individually poses philosophical
% challenges for a Bayesian treatment of this method since beliefs are being
% updated for different models.

% TODO elaborate this from Tokdar et al., 2012

\section{A Different Approach}

We propose a new approach to modelling distributions which is inspired by the
quantile regression framework.

Suppose that the probability of $y$ is found as the following density:
\begin{align*}
  P(y) = \frac{1}{Z}\prod^N_{i=1}P_i(y),
\end{align*}
for $Z$ some normalising term and each $P_i(y)$ of the form $\exp\{-g(y)\}$ for
$g(\cdot)$ a strictly positive real function. Taking logarithms, we may express
the log-probability of $y$, $\mathcal{L}(y) = \log P(y)$ as the following sum of
terms:
\[
  \mathcal{L}(y) = \sum^N_{i=1}\mathcal{L}_i(y) - \log Z.
\]
Drawing inspiration from (\ref{eq:exploss}), where the residuals are weighted
differently for positive and negative residuals, we define each term to be of
the form:
\[
  \mathcal{L}_i(y) = -(y-f_i)\Big(\alpha_i \Theta(y-f_i) - \beta_i
    \Theta(f_i-y)\Big),
\]
for $\Theta$ a Heaviside step function, and $\alpha_i$, $\beta_i$ and $f_i$ some
trainable parameters. We assume $\alpha_i, \beta_i > 0$ for all $i \in [1,N]$,
and that the $f_i$ are ordered, such that $f_i \le f_{i+1}$.

% TODO insert diagram here of each individual loss term.

To calculate the value of the normalising term, first consider an expression for
the segment where $y \in (f_j, f_{j+1}]$
\begin{align*}
  \mathcal{L}(f_j < y \le f_{j+1}) &= -\sum_{i=1}^j \alpha_i(y-f_i) +
  \sum_{i=j+1}^N \beta_i(y-f_i) - \log Z \\
                                   &= \left(\sum_{i=j+1}^N \beta_i -
                                     \sum_{i=1}^j\alpha_i\right)y +
                                     \left(\sum_{i=1}^j\alpha_if_i -
                                       \sum_{i=j+1}^N\beta_if_i\right) - \log Z
                                       \\
                                   &\doteq a_j y + b_j - \log Z
\end{align*}
Where we have defined
\begin{equation}\label{eq:adef}
  a_j = \sum^N_{i=j+1}\beta_i - \sum^j_{i=1}\alpha_i
\end{equation}
and
\begin{equation}\label{eq:bdef}
  b_j = \sum^j_{i=1}\alpha_if_i - \sum^N_{i=j+1}\beta_if_i.
\end{equation}
We can also consider the segment where $y\in(-\infty, f_1]$:
\begin{align*}
  \mathcal{L}(-\infty < y \le f_{1}) &= \sum^N_{i=1}\beta_i(y-f_i) - \log Z \\
                                     &= \underbrace{\sum^N_{i=1}\beta_i}_{a_0} y -
                                     \underbrace{\sum^N_{j=1}\beta_i f_i}_{b_0} - \log Z
\end{align*}
as well as the segment for $y \in (f_N, \infty)$:
\begin{align*}
  \mathcal{L}(f_N < y < \infty) &= -\left(\sum^N_{i=1}\alpha_i(y-f_i)\right) -
  \log Z \\
                                &= \underbrace{-\sum^N_{i=1}\alpha_i}_{a_N}y +
                                \underbrace{\sum^N_{i=1}\alpha_if_i}_{b_N} -
                                \log Z,
\end{align*}
where we can see that $a_0, b_0, a_N$ and $b_N$ are consistent with the
definition of $a_j$ in Eq.~\ref{eq:adef} and $b_j$ in Eq.~\ref{eq:bdef}.

Now the normalising term $Z$ can be found by summing the integrals of each of
these line segments. Beginning with the case where $f_j < y \le f_{j+1}$:
\begin{align*}
  \int_{f_j}^{f_{j+1}}e^{a_j y + b_j}dy &=
  \left[\frac{1}{a_j}e^{a_jy+b_j}\right]^{f_{j+1}}_{f_j} \\
                                        &=
                                        \frac{1}{a_j}e^b_j\left(e^{a_jf_{j+1}}-e^{a_jf_j}\right).
\end{align*}
When $y \in (-\infty, f_1]$, we have
\begin{align*}
  \lim_{\ell\to-\infty} \int^{f_1}_{\ell}e^{a_0y+b_0}dy &= \lim_{\ell\to-\infty} \left[\frac{1}{a_0}e^{a_0y+b_0}\right]^{f_1}_{\ell} \\
                                                  &= \lim_{\ell\to-\infty} \frac{1}{a_0}e^{b_0}\left(e^{a_0f_1}-e^{a_0\ell}) \\
                                                  &= \frac{1}{a_0}e^{a_0f_1+b_0},
\end{align*}
since $a_0>0$. Similarly for the segment where $y\in (f_N, \infty)$,
\begin{align*}
  \lim_{\ell\to\infty} \int^{\ell}_{f_N}e^{a_Ny+b_N}dy &=
  \lim_{\ell\to\infty}\left[\frac{1}{a_N}e^{a_Ny+b_N}\right]^{\ell}_{f_N} \\
                                               &=
                                               \lim_{\ell\to\infty}\frac{1}{a_N}e^{b_N}\left(e^{a_N\ell}-e^{a_Nf_N})
                                                 \\
                                               &= -\frac{1}{a_N}e^{a_Nf_N+b_N}
\end{align*}
since $a_N < 0$.
We can now explicitly write down the normalising term $Z$ as the sum of these
integrals:
\begin{align*}
  Z &= \int_{-\infty}^{f_1}e^{a_0y+b_0}dy +
  \sum^N_{j=1}\int^{f_{j+1}}_{f_j}e^{a_jy+b_j}dy +
  \int^{\infty}_{f_N}e^{a_Ny+b_N}dy \\
    &= \frac{1}{a_0}e^{a_0f_1+b_0} +
    \sum^N_{j=1}\frac{1}{a_j}e^{b_j}\left(e^{a_jf_{j+1}}-e^{a_jf_j}\right) -
    \frac{1}{a_N}e^{a_Nf_N+b_N}
\end{align*}

\subsection{Efficient Implementation}

A na\"ive computation of $Z$ would run in $O(n^2)$ time, however with a
very simple dynamic programming approach, where we compute partial sums of
$\alpha_i$ and $\beta_i$ as well as $\alpha_if_i$ and $\beta_if_i$ ahead of
time, we can find $Z$ in just $O(n)$ time. (See notebook for details).

\newpage

\bibliography{library.bib}

\end{document}

