{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Gamma\n",
    "\n",
    "import bayesfunc as bf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from ipywidgets import interact_manual, interact, FloatSlider, IntSlider\n",
    "\n",
    "dtype=t.float64\n",
    "device=\"cpu\"\n",
    "#device=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2631e966d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_features = 1\n",
    "out_features = 1\n",
    "train_batch = 40\n",
    "batches = 3\n",
    "data_size = batches * train_batch\n",
    "\n",
    "t.manual_seed(0)\n",
    "\n",
    "def generate_data():\n",
    "    noise = Gamma(2, 0.1)\n",
    "    X = t.rand(data_size, in_features) * 4 - 2\n",
    "    x_1 = X[:int(data_size/2), :]\n",
    "    x_2 = X[int(data_size/2):, :]\n",
    "    \n",
    "    y = t.cat((\n",
    "        x_1**3 + 8 + 1 * noise.sample((int(data_size/2), in_features)),\n",
    "        x_2**3 - 8 - 0.2 * noise.sample((int(data_size/2), in_features))\n",
    "    ))\n",
    "    \n",
    "    scale = y.std()\n",
    "    y = y/scale\n",
    "    \n",
    "    xys = t.cat((X,y),1)\n",
    "    xys = xys[t.randperm(xys.size()[0])]\n",
    "    X = xys[:,0].unsqueeze(1).to(device=device, dtype=dtype)\n",
    "    y = xys[:,1].unsqueeze(1).to(device=device, dtype=dtype)\n",
    "    return X, y, scale\n",
    "\n",
    "X, y, scale = generate_data()\n",
    "\n",
    "plt.scatter(X.detach().cpu(), y.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the probability of the outcome variable $y$ as\n",
    "$$\n",
    "P(y) = \\frac{1}{Z}\\prod^N_{i=1}P_i(y),\n",
    "$$\n",
    "writing the corresponding log-probability as\n",
    "$$\n",
    "\\mathcal{L}(y) = \\sum^N_{i=1}\\mathcal{L}_i(y) - \\log Z\n",
    "$$\n",
    "where each $L_i(y)$ is of the form:\n",
    "$$\n",
    "\\mathcal{L}(y) = \\sum^N_{i=1} - (y-f_i)\\left(\\alpha_i \\Theta(y-f_i) - \\beta_i\\Theta(y-f_i)\\right) - \\log Z,\n",
    "$$\n",
    "where $\\Theta(u)$ is Heaviside step with value $1$ if $u>0$ and $0$ otherwise, $\\alpha_i > 0$ and $\\beta_i > 0$ for all $i \\in [1, N]$ and we also assume $f_i$ are ordered; that is, $f_i \\le f_{i+1}$ for all $i \\in [1,N]$. Crucially each of these terms in the summand is negative for all values of $y$, hence the probability is a valid member of the exponential family of distributions.\n",
    "\n",
    "The normalising term $Z$ is found as\n",
    "$$\n",
    "Z = \\frac{1}{a_0}e^{a_0f_1+b_0} + \\sum^{N-1}_{j=1}\\frac{1}{a_j}e^{b_j}\\left(e^{a_jf_{j+1}}-e^{a_jf_j}\\right) - \\frac{1}{a_N}e^{a_Nf_N + b_N},\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "a_j &= \\sum^N_{i=j+1}\\beta_i - \\sum^j_{i=1}\\alpha_i \\\\\n",
    "b_j &= \\sum^j_{i=1}\\alpha_if_i - \\sum^N_{i=j+1}\\beta_if_i,\n",
    "\\end{align*}\n",
    "In particular, we have the following edge cases which can be written explicitly; for $j=0$:\n",
    "$$\n",
    "a_0 = \\sum^N_{i=1}\\beta_i,\\ \\ \\ \\ b_0 = -\\sum^N_{i=1}\\beta_if_i\n",
    "$$\n",
    "and for $j=N$:\n",
    "$$\n",
    "a_N = -\\sum^N_{i=1}\\alpha_i,\\ \\ \\ \\ b_N = \\sum^N_{i=1}\\alpha_if_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enforce the constraints on the values of $\\mathbf{f}$, $\\mathbf{\\alpha}$ and $\\mathbf{\\beta}$, we first ensure that each are positive. Let the $\\mathbf{f}$ values returned from the network be denoted $\\hat{f}$. The ordering constraint on $f$ is enforced by additionally outputting a base value $r$ to which $\\hat{f}_1$ is added, giving $f_1 = r + \\hat{f}_1$. Subsequent $f_i$ for $i > 1$ can be recovered by finding the cumulative sum $f_i = f_{i-1} + \\hat{f}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can devise a simple dynamic programming algorithm to compute these normalising terms $a_j$ and $b_j$ efficiently by computing vectors of their partial sums in $O(n)$ time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc4741078074a1c9407947a59411c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inducing_batch = 40\n",
    "\n",
    "# number of fs, alphas and betas\n",
    "N = 10\n",
    "\n",
    "# Select Data\n",
    "data_X = X\n",
    "data_y = y\n",
    "scale = scale\n",
    "\n",
    "class NET(nn.Module):\n",
    "    def __init__(self, inducing_batch, N):\n",
    "        hidden = 50\n",
    "        super(NET, self).__init__()\n",
    "        inducing_data = t.linspace(data_X.min(), data_X.max(), inducing_batch).unsqueeze(1)\n",
    "        self.ia = bf.InducingAdd(inducing_batch=inducing_batch, inducing_data=inducing_data)\n",
    "        self.ir = bf.InducingRemove(inducing_batch=inducing_batch)\n",
    "        \n",
    "        self.fc1 = bf.GILinear(in_features=1, out_features=hidden, inducing_batch=inducing_batch, bias=True)\n",
    "        self.offset = bf.GILinear(in_features=hidden, out_features=1, inducing_batch=inducing_batch, bias=True, full_prec=True)\n",
    "        self.f = bf.GILinear(in_features=hidden, out_features=N, inducing_batch=inducing_batch, bias=True, full_prec=True)\n",
    "        self.alpha = bf.GILinear(in_features=hidden, out_features=N-1, inducing_batch=inducing_batch, bias=True, full_prec=True)\n",
    "        self.beta = bf.GILinear(in_features=hidden, out_features=N-1, inducing_batch=inducing_batch, bias=True, full_prec=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ia(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # this can be positive or negative\n",
    "        offset = self.ir(self.offset(x))\n",
    "        # the following _must_ be non-negative, here we use softplus\n",
    "        f = self.ir(F.softplus(self.f(x)))\n",
    "        alpha = self.ir(self.alpha(x))\n",
    "        beta = self.ir(self.beta(x))\n",
    "        return offset, f, alpha, beta\n",
    "    \n",
    "net = NET(inducing_batch, N).to(device=device, dtype=dtype)\n",
    "\n",
    "samples = 10\n",
    "opt = t.optim.Adam(net.parameters(), lr=0.05)\n",
    "for i in tqdm(range(200)):\n",
    "    for batch in range(batches):\n",
    "        l = batch * train_batch\n",
    "        u = l     + train_batch\n",
    "        batch_X = data_X[l:u].expand(samples, -1, -1)\n",
    "        batch_y = data_y[l:u].expand(samples, -1, -1)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        (offset, f, alpha, beta), logpq, _ = bf.propagate(net, batch_X)\n",
    "        \n",
    "        # Calculate beta_1 term:\n",
    "        beta = t.cat((\n",
    "            ((-beta.sum(2))+0.01).unsqueeze(2),\n",
    "            beta\n",
    "        ), 2)\n",
    "        # Calculate alpha_N term\n",
    "        alpha = t.cat((\n",
    "            alpha,\n",
    "            ((-alpha.sum(2))+0.01).unsqueeze(2)\n",
    "        ), 2)\n",
    "        \n",
    "        # Add an offset to f_0 for all samples.\n",
    "        # TODO is there a cleaner / more efficient way to do this?\n",
    "        pad = t.cat((\n",
    "            offset,\n",
    "            t.zeros_like(offset).expand(-1, -1, N-2).to(dtype=dtype, device=device)\n",
    "        ),2)\n",
    "        f_2 = f[:,:,1:] + pad\n",
    "        \n",
    "        # Find the cumulative sum to ensure f_i <= f_{i+1}\n",
    "        f = t.cat((offset - f[:,:,0].unsqueeze(2), f_2.cumsum(2)),2)\n",
    "        \n",
    "        # Log-likelihood terms\n",
    "        res = batch_y - f\n",
    "        ll_terms = -(res) * t.where(res > 0, alpha, -beta)\n",
    "                \n",
    "        # Calculating the normalising term ------------------------------------\n",
    "        \n",
    "        # Flip these matrices along third dimension by copying memory; O(n)\n",
    "        b_flip = beta.flip(2)\n",
    "        f_flip = f.flip(2)\n",
    "        \n",
    "        # Calculate vectors of partial sums ('_ps') in O(n) time\n",
    "        a_ps = alpha.cumsum(2)\n",
    "        b_ps = b_flip.cumsum(2).flip(2)\n",
    "        af_ps = (alpha * f).cumsum(2)\n",
    "        bf_ps = (b_flip * f_flip).cumsum(2).flip(2)\n",
    "        \n",
    "        # Calculate main coefficient vectors\n",
    "        a_vec  = b_ps[:,:,1:] - a_ps[:,:,:-1]\n",
    "        b_vec  = af_ps[:,:,:-1] - bf_ps[:,:,1:]\n",
    "        \n",
    "        # Calculate terms for interval y \\in (-\\infty, f_1]\n",
    "        a_0 = b_ps[:,:,0]\n",
    "        b_0 = - bf_ps[:,:,0]\n",
    "        a_N = - a_ps[:,:,-1]\n",
    "        b_N = af_ps[:,:,-1]\n",
    "        \n",
    "        # Calculate the normalising term\n",
    "        Z = 1/a_0 * t.exp(a_0 * f[:,:,0] + b_0)\n",
    "        Z = Z + (1/a_vec * t.exp(b_vec)*(t.exp(a_vec * f[:,:,1:])-t.exp(a_vec*f[:,:,:-1]))).sum(2)\n",
    "        Z = Z - (1/a_N * t.exp(a_N * f[:,:,N-1] + b_N))\n",
    "        \n",
    "        lls = ll_terms.sum(-1) - t.log(Z)\n",
    "        ll = lls.mean(-1)\n",
    "        assert ll.shape == (samples,)\n",
    "        assert logpq.shape == (samples,)\n",
    "        elbo = ll + logpq/data_size\n",
    "        \n",
    "        (-elbo.mean()).backward()\n",
    "        opt.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741a9b70533b4fe5b446314e12e82ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x_loc', max=1.9882662296295166, min=-1.9950559139251â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_density(x_loc)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pts = 100\n",
    "\n",
    "def plot_density(x_loc):\n",
    "    # The number of points to plot the quantile function at (ys)\n",
    "    with t.no_grad():\n",
    "        samples = 100\n",
    "        xs = t.tensor([x_loc]).unsqueeze(1).to(dtype=dtype, device=device)\n",
    "        ys = t.linspace(-5, 5, num_pts).unsqueeze(1).to(dtype=dtype, device=device)\n",
    "        xs = xs.expand(samples, -1, -1)\n",
    "        ys = ys.expand(samples, -1, -1)\n",
    "        (offset, f, alpha, beta), _, _ = bf.propagate(net, xs)\n",
    "        \n",
    "        # Calculate beta_1 term:\n",
    "        beta = t.cat((\n",
    "            (-beta.sum(2)+0.01).unsqueeze(2),\n",
    "            beta\n",
    "        ), 2)\n",
    "        # Calculate alpha_N term\n",
    "        alpha = t.cat((\n",
    "            alpha,\n",
    "            (-alpha.sum(2)+0.01).unsqueeze(2)\n",
    "        ), 2)\n",
    "        \n",
    "        # Add an offset to f_0 for all samples.\n",
    "        # TODO is there a cleaner / more efficient way to do this?\n",
    "        pad = t.cat((\n",
    "            offset,\n",
    "            t.zeros_like(offset).expand(-1, -1, N-2).to(dtype=dtype, device=device)\n",
    "        ),2)\n",
    "        f_2 = f[:,:,1:] + pad\n",
    "        \n",
    "        # Find the cumulative sum to ensure f_i <= f_{i+1}\n",
    "        f = t.cat((offset - f[:,:,0].unsqueeze(2), f_2.cumsum(2)),2)\n",
    "        \n",
    "        # Log-likelihood terms\n",
    "        res = ys - f\n",
    "        # TODO exponentiate here\n",
    "        ll_terms = -(res) * t.where(res > 0, alpha, -beta)\n",
    "        \n",
    "        # Calculating the normalising term ------------------------------------\n",
    "        \n",
    "        # Flip these matrices along third dimension by copying memory; O(n)\n",
    "        b_flip = beta.flip(2)\n",
    "        f_flip = f.flip(2)\n",
    "        \n",
    "        # Calculate vectors of partial sums O(n)\n",
    "        a_ps = alpha.cumsum(2)\n",
    "        b_ps = b_flip.cumsum(2).flip(2)\n",
    "        af_ps = (alpha*f).cumsum(2)\n",
    "        bf_ps = (b_flip * f_flip).cumsum(2).flip(2)\n",
    "\n",
    "        # Calculate main coefficient vectors\n",
    "        a_vec  = b_ps[:,:,1:] - a_ps[:,:,:-1]\n",
    "        b_vec  = af_ps[:,:,:-1] - bf_ps[:,:,1:]\n",
    "        \n",
    "        # Calculate terms for interval y \\in (-\\infty, f_1]\n",
    "        a_0 = b_ps[:,:,0]\n",
    "        b_0 = - bf_ps[:,:,0]\n",
    "        a_N = - a_ps[:,:,-1]\n",
    "        b_N = af_ps[:,:,-1]\n",
    "        \n",
    "        # Calculate the normalising term\n",
    "        Z = 1/a_0 * t.exp(a_0 * f[:,:,0] + b_0)\n",
    "        Z = Z + (1/a_vec * t.exp(b_vec)*(t.exp(a_vec * f[:,:,1:])-t.exp(a_vec*f[:,:,:-1]))).sum(2)\n",
    "        Z = Z - (1/a_N * t.exp(a_N * f[:,:,N-1] + b_N))\n",
    "                        \n",
    "        ls = t.exp(ll_terms.sum(-1) - t.log(Z))\n",
    "        l = ls.mean(0).detach().cpu()\n",
    "        std = ls.std(0).detach().cpu()\n",
    "        \n",
    "        plt.plot(ys[0].detach().cpu(), l)\n",
    "        plt.fill_between(ys[0].detach().cpu().squeeze(), (l+2*std).squeeze(), (l-2*std).squeeze(), alpha=0.2)\n",
    "        plt.title(f\"Distribution at x={x_loc}\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.xlabel(\"Y value\")\n",
    "\n",
    "interact(plot_density, x_loc=FloatSlider(min=data_X.min(), max=data_X.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
